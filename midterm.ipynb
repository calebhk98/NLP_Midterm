{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1890007b",
   "metadata": {},
   "source": [
    "I. Introduction - - - - \n",
    "Domain-specific area \n",
    "Objectives \n",
    "Dataset Description \n",
    "Evaluation methodology \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8599490",
   "metadata": {},
   "source": [
    "II. Implementation - - - - \n",
    "Data Preprocessing \n",
    "Baseline Performance \n",
    "Comparative Classification methodology \n",
    "Programming style \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b773f",
   "metadata": {},
   "source": [
    "III. Conclusions - - \n",
    "Performance Analysis & Comparative Discussion \n",
    "Project Summary and Reflections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a2155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/NLP_Midterm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed and imported modules\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets matplotlib pandas seaborn scikit-learn nltk transformers torch accelerate\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "print(\"Installed and imported modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff108f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e132e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30d860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20156e",
   "metadata": {},
   "source": [
    "Ok, let's look at this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ab9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 25000\n",
      "Test size: 25000\n",
      "\n",
      "Class distribution in training set:\n",
      "label\n",
      "0    12500\n",
      "1    12500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in test set:\n",
      "label\n",
      "0    12500\n",
      "1    12500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Review #1:\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attent\n",
      "\n",
      "Label: 0 (0=negative, 1=positive)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_df[\"label\"].value_counts())\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(test_df[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nReview #1:\")\n",
    "print(train_df.iloc[0][\"text\"][:500])\n",
    "print(\"\\nLabel:\", train_df.iloc[0][\"label\"], \"(0=negative, 1=positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4e5e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text (first 200 chars):\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ev\n",
      "\n",
      "Cleaned text (first 200 chars):\n",
      "rented curiousyellow video store controversy surrounded first released also heard first seized us customs ever tried enter country therefore fan films considered controversial really see myselfthe plo\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "\t\"\"\"\n",
    "\tPreprocesses text for NLP tasks\n",
    "\tSteps: lowercase, remove HTML/special chars, tokenize, remove stopwords\n",
    "\t\"\"\"\n",
    "\t# Lowercase\n",
    "\ttext = text.lower()\n",
    "\t\n",
    "\t# Remove HTML tags\n",
    "\ttext = re.sub(r\"<.*?>\", \"\", text)\n",
    "\t\n",
    "\t# Remove special characters and digits\n",
    "\ttext = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\t\n",
    "\t# Tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\t\n",
    "\t# Remove stopwords using explicit loop\n",
    "\tstop_words = set(stopwords.words(\"english\"))\n",
    "\tfiltered_tokens = []\n",
    "\tfor word in tokens:\n",
    "\t\tif word not in stop_words:\n",
    "\t\t\tfiltered_tokens.append(word)\n",
    "\t\n",
    "\treturn \" \".join(filtered_tokens)\n",
    "\n",
    "# Test the function on one review\n",
    "sample_text = train_df.iloc[0][\"text\"]\n",
    "print(\"Original text (first 200 chars):\")\n",
    "print(sample_text[:200])\n",
    "print(\"\\nCleaned text (first 200 chars):\")\n",
    "print(preprocess_text(sample_text)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train data\n",
      "Train data preprocessed\n",
      "\n",
      "Preprocessing test data\n",
      "Test data preprocessed\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing train data. Wait  17 seconds.\")\n",
    "train_df[\"cleaned_text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "print(\"Train data preprocessed\")\n",
    "\n",
    "print(\"\\nPreprocessing test data. Wait another 17 seconds.\")\n",
    "test_df[\"cleaned_text\"] = test_df[\"text\"].apply(preprocess_text)\n",
    "print(\"Test data preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31cd39",
   "metadata": {},
   "source": [
    "Let's get a baseline. We could say a baseline is 50%, as it is evenly split, but that might be disingenuous.  So I'll do a bag of words, which while looking for a baseline, was in the results several times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8143d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting, wait 5 seconds.\n",
      "Shape: (25000, 5000)\n",
      "\n",
      "Baseline:\n",
      "Accuracy:  0.8491\n",
      "Precision: 0.8535\n",
      "Recall:    0.8429\n",
      "F1-Score:  0.8482\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.86      0.85     12500\n",
      "    Positive       0.85      0.84      0.85     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10692,  1808],\n",
       "       [ 1964, 10536]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting, wait 5 seconds.\")\n",
    "# CountVectorizer creates simple word count features\n",
    "# max_features=5000: Only use the 5000 most common words\n",
    "bagOfWordsVectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_bow = bagOfWordsVectorizer.fit_transform(train_df[\"cleaned_text\"])\n",
    "X_test_bow = bagOfWordsVectorizer.transform(test_df[\"cleaned_text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "print(\"Shape:\", X_train_bow.shape)\n",
    "\n",
    "# Train Logistic Regression\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=147)\n",
    "baseline_model.fit(X_train_bow, y_train)\n",
    "\n",
    "# Make predictions\n",
    "baseline_predictions = baseline_model.predict(X_test_bow)\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_predictions)\n",
    "baseline_precision = precision_score(y_test, baseline_predictions)\n",
    "baseline_recall = recall_score(y_test, baseline_predictions)\n",
    "baseline_f1 = f1_score(y_test, baseline_predictions)\n",
    "\n",
    "print(\"\\nBaseline:\")\n",
    "print(\"Accuracy: \", round(baseline_accuracy, 4))\n",
    "print(\"Precision:\", round(baseline_precision, 4))\n",
    "print(\"Recall:   \", round(baseline_recall, 4))\n",
    "print(\"F1-Score: \", round(baseline_f1, 4))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, baseline_predictions, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "# Save confusion matrix for later visualization\n",
    "baseline_cm = confusion_matrix(y_test, baseline_predictions)\n",
    "baseline_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa3f7f",
   "metadata": {},
   "source": [
    "Ok, so about ~85%, much better than 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6158ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features, wait 13 seconds\n",
      "TF-IDF matrix shape: (25000, 5000)\n",
      "Number of features: 5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# What: TF-IDF converts text to numbers for machine learning\n",
    "#       - TF (Term Frequency): How often a word appears in a document\n",
    "#       - IDF (Inverse Document Frequency): How rare/important a word is\n",
    "#       - Creates 5000 features (most important words/bigrams)\n",
    "\n",
    "print(\"Creating TF-IDF features, wait 13 seconds\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"cleaned_text\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df[\"cleaned_text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_train_tfidf.shape)\n",
    "print(\"Number of features:\", X_train_tfidf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STATISTICAL MODEL (Naive Bayes + TF-IDF)\n",
      "Accuracy:  0.8501\n",
      "Precision: 0.8508\n",
      "Recall:    0.8491\n",
      "F1-Score:  0.85\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.85      0.85     12500\n",
      "    Positive       0.85      0.85      0.85     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Train Naive Bayes Classifier\n",
    "# Section: Comparative Classification (Section 7) - Statistical Model\n",
    "# What: Naive Bayes is a probabilistic classifier\n",
    "#       Works well with text data and TF-IDF features\n",
    "#       Fast to train and interpret\n",
    "# =============================================================================\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nb_predictions = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_precision = precision_score(y_test, nb_predictions)\n",
    "nb_recall = recall_score(y_test, nb_predictions)\n",
    "nb_f1 = f1_score(y_test, nb_predictions)\n",
    "\n",
    "print(\"\\nSTATISTICAL MODEL (Naive Bayes + TF-IDF)\")\n",
    "print(\"Accuracy: \", round(nb_accuracy, 4))\n",
    "print(\"Precision:\", round(nb_precision, 4))\n",
    "print(\"Recall:   \", round(nb_recall, 4))\n",
    "print(\"F1-Score: \", round(nb_f1, 4))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, nb_predictions, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "# Save confusion matrix for later\n",
    "nb_cm = confusion_matrix(y_test, nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "No GPU detected. Training will be slower.\n",
      "Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Setup BERT model and tokenizer\n",
    "# Section: Comparative Classification (Section 7) - Embedding Model\n",
    "# What: Loading DistilBERT (smaller, faster version of BERT)\n",
    "#       - BERT uses word embeddings (dense vector representations)\n",
    "#       - Pre-trained on massive text corpus\n",
    "#       - We'll fine-tune it on our sentiment data\n",
    "# =============================================================================\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "\tprint(\"GPU detected! Training will be faster.\")\n",
    "else:\n",
    "\tprint(\"No GPU detected. Training will be slower.\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4437e0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 5000 samples\n",
      "Testing on 2500 samples\n",
      "Datasets prepared\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Prepare dataset for BERT\n",
    "# Section: Comparative Classification (Section 7) - Embedding Model\n",
    "# What: Converting our data to format BERT expects\n",
    "#       Using smaller subset (5000 train, 2500 test) for speed\n",
    "#       Remove these lines if you have GPU and want to use full dataset\n",
    "# =============================================================================\n",
    "\n",
    "# OPTION 1: Use smaller subset for speed (recommended for CPU)\n",
    "train_subset = train_df.sample(n=5000, random_state=147)\n",
    "test_subset = test_df.sample(n=2500, random_state=147)\n",
    "\n",
    "# OPTION 2: Use full dataset (comment out lines above and uncomment these)\n",
    "# train_subset = train_df\n",
    "# test_subset = test_df\n",
    "\n",
    "print(\"Training on\", len(train_subset), \"samples\")\n",
    "print(\"Testing on\", len(test_subset), \"samples\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_subset[[\"text\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_subset[[\"text\", \"label\"]])\n",
    "\n",
    "print(\"Datasets prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:17<00:00, 286.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [00:11<00:00, 208.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Tokenize data for BERT\n",
    "# Section: Comparative Classification (Section 7) - Embedding Model\n",
    "# What: BERT needs text converted to token IDs\n",
    "#       Padding/truncating to max length of 512 tokens\n",
    "# =============================================================================\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\treturn tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "print(\"Tokenizing training data. Wait 15 seconds.\")\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "print(\"Tokenizing test data. Wait another 15 seconds. \")\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (this will take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/NLP_Midterm/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Setup and train BERT model\n",
    "# Section: Comparative Classification (Section 7) - Embedding Model\n",
    "# What: Fine-tuning DistilBERT on our sentiment data\n",
    "#       This will take 10-30 minutes depending on your hardware\n",
    "#       2 epochs = going through the entire dataset twice\n",
    "# =============================================================================\n",
    "\n",
    "def setupModel():\n",
    "\t# Define where to save the model\n",
    "\tmodel_save_path = \"./saved_distilbert_model\"\n",
    "\ttrainOverSave=False\n",
    "\n",
    "\t# Check if we already have a trained model saved\n",
    "\tif os.path.exists(model_save_path) and not trainOverSave:\n",
    "\t\tprint(\"Found saved model! Loading it instead of training...\")\n",
    "\t\tmodel = DistilBertForSequenceClassification.from_pretrained(model_save_path)\n",
    "\t\tmodel.to(device)\n",
    "\t\tprint(\"Model loaded successfully!\")\n",
    "\t\treturn model\n",
    "\n",
    "\n",
    "\t# Load pre-trained model\n",
    "\tmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\tmodel.to(device)\n",
    "\n",
    "\t# Training configuration\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\toutput_dir=\"./results\",\n",
    "\t\tnum_train_epochs=2,\n",
    "\t\tper_device_train_batch_size=16,\n",
    "\t\tper_device_eval_batch_size=16,\n",
    "\t\twarmup_steps=500,\n",
    "\t\tweight_decay=0.01,\n",
    "\t\tlogging_dir=\"./logs\",\n",
    "\t\tlogging_steps=100,\n",
    "\t\teval_strategy=\"epoch\",\n",
    "\t\tsave_strategy=\"epoch\",\n",
    "\t\tload_best_model_at_end=True,\n",
    "\t)\n",
    "\n",
    "\t# Create trainer\n",
    "\ttrainer = Trainer(\n",
    "\t\tmodel=model,\n",
    "\t\targs=training_args,\n",
    "\t\ttrain_dataset=train_tokenized,\n",
    "\t\teval_dataset=test_tokenized,\n",
    "\t)\n",
    "\n",
    "\t# Train the model\n",
    "\tprint(\"Starting training... (this will take a while)\")\n",
    "\ttrainer.train()\n",
    "\tprint(\"Training complete!\")\n",
    "\n",
    "\t# Save the trained model\n",
    "\tprint(\"Saving model to\", model_save_path)\n",
    "\tmodel.save_pretrained(model_save_path)\n",
    "\ttokenizer.save_pretrained(model_save_path)\n",
    "\treturn model\n",
    "model = setupModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74467e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Evaluate BERT model\n",
    "# Section: Comparative Classification (Section 7) - Embedding Model\n",
    "# What: Getting predictions and calculating metrics for BERT\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=TrainingArguments(output_dir=\"./results\", per_device_eval_batch_size=16),\n",
    ")\n",
    "\n",
    "print(\"Getting predictions...\")\n",
    "bert_predictions_output = trainer.predict(test_tokenized)\n",
    "bert_predictions = np.argmax(bert_predictions_output.predictions, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "bert_accuracy = accuracy_score(test_subset[\"label\"], bert_predictions)\n",
    "bert_precision = precision_score(test_subset[\"label\"], bert_predictions)\n",
    "bert_recall = recall_score(test_subset[\"label\"], bert_predictions)\n",
    "bert_f1 = f1_score(test_subset[\"label\"], bert_predictions)\n",
    "\n",
    "print(\"\\nEMBEDDING MODEL (DistilBERT)\")\n",
    "print(\"Accuracy: \", round(bert_accuracy, 4))\n",
    "print(\"Precision:\", round(bert_precision, 4))\n",
    "print(\"Recall:   \", round(bert_recall, 4))\n",
    "print(\"F1-Score: \", round(bert_f1, 4))\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_subset[\"label\"], bert_predictions, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "# Save confusion matrix\n",
    "bert_cm = confusion_matrix(test_subset[\"label\"], bert_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addf01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: Compare all models\n",
    "# Section: Performance Analysis (Section 9 in PDF)\n",
    "# What: Creating summary table of all model results\n",
    "# =============================================================================\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "\t\"Model\": [\"Baseline\", \"Naive Bayes\", \"DistilBERT\"],\n",
    "\t\"Accuracy\": [baseline_accuracy, nb_accuracy, bert_accuracy],\n",
    "\t\"Precision\": [baseline_precision, nb_precision, bert_precision],\n",
    "\t\"Recall\": [baseline_recall, nb_recall, bert_recall],\n",
    "\t\"F1-Score\": [baseline_f1, nb_f1, bert_f1]\n",
    "})\n",
    "\n",
    "print(\"\\nFINAL RESULTS COMPARISON\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: Create visualizations\n",
    "# Section: Performance Analysis (Section 9 in PDF)\n",
    "# What: Creating comparison charts and confusion matrices\n",
    "#       This generates publication-quality figures for your report\n",
    "# =============================================================================\n",
    "\n",
    "# Create a figure with 4 subplots arranged in 2 rows and 2 columns\n",
    "# figsize=(15, 12) means width=15 inches, height=12 inches\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# ============================================================================\n",
    "# CHART 1: Bar chart comparing all metrics across all models\n",
    "# Located at position [0, 0] (top-left)\n",
    "# ============================================================================\n",
    "ax1 = axes[0, 0]\n",
    "\n",
    "# Create x-axis positions for each model (0, 1, 2 for Baseline, NB, BERT)\n",
    "x = np.arange(len(results_df))\n",
    "\n",
    "# Width of each bar (0.2 so we can fit 4 bars side by side)\n",
    "width = 0.2\n",
    "\n",
    "# Create 4 sets of bars, one for each metric\n",
    "# x - width*1.5: Position first bar (Accuracy) to the left\n",
    "# x - width*0.5: Position second bar (Precision) slightly left of center\n",
    "# x + width*0.5: Position third bar (Recall) slightly right of center\n",
    "# x + width*1.5: Position fourth bar (F1) to the right\n",
    "ax1.bar(x - width*1.5, results_df[\"Accuracy\"], width, label=\"Accuracy\", alpha=0.8)\n",
    "ax1.bar(x - width*0.5, results_df[\"Precision\"], width, label=\"Precision\", alpha=0.8)\n",
    "ax1.bar(x + width*0.5, results_df[\"Recall\"], width, label=\"Recall\", alpha=0.8)\n",
    "ax1.bar(x + width*1.5, results_df[\"F1-Score\"], width, label=\"F1-Score\", alpha=0.8)\n",
    "\n",
    "# Set labels for x and y axes\n",
    "ax1.set_xlabel(\"Model\", fontsize=12)\n",
    "ax1.set_ylabel(\"Score\", fontsize=12)\n",
    "\n",
    "# Set title for this chart\n",
    "ax1.set_title(\"Model Performance Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Set x-axis tick positions and labels\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df[\"Model\"])\n",
    "\n",
    "# Add legend to show which color represents which metric\n",
    "ax1.legend()\n",
    "\n",
    "# Set y-axis limits from 0 to 1 (since all metrics are between 0 and 1)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Add horizontal grid lines for easier reading (alpha=0.3 makes them faint)\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# ============================================================================\n",
    "# CHART 2: Confusion matrix heatmap for Naive Bayes\n",
    "# Located at position [0, 1] (top-right)\n",
    "# ============================================================================\n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "# Create heatmap using seaborn\n",
    "# nb_cm: confusion matrix data (2x2 array)\n",
    "# annot=True: Show numbers in each cell\n",
    "# fmt=\"d\": Format numbers as integers\n",
    "# cmap=\"Blues\": Use blue color scheme\n",
    "# xticklabels/yticklabels: Label the axes\n",
    "sns.heatmap(nb_cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax2, \n",
    "\t\t\txticklabels=[\"Negative\", \"Positive\"], \n",
    "\t\t\tyticklabels=[\"Negative\", \"Positive\"])\n",
    "\n",
    "# Set title and axis labels\n",
    "ax2.set_title(\"Confusion Matrix - Naive Bayes\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"True Label\", fontsize=12)\n",
    "ax2.set_xlabel(\"Predicted Label\", fontsize=12)\n",
    "\n",
    "# ============================================================================\n",
    "# CHART 3: Confusion matrix heatmap for DistilBERT\n",
    "# Located at position [1, 0] (bottom-left)\n",
    "# ============================================================================\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Similar to Chart 2 but with green color scheme and BERT data\n",
    "sns.heatmap(bert_cm, annot=True, fmt=\"d\", cmap=\"Greens\", ax=ax3,\n",
    "\t\t\txticklabels=[\"Negative\", \"Positive\"], \n",
    "\t\t\tyticklabels=[\"Negative\", \"Positive\"])\n",
    "\n",
    "ax3.set_title(\"Confusion Matrix - DistilBERT\", fontsize=14, fontweight=\"bold\")\n",
    "ax3.set_ylabel(\"True Label\", fontsize=12)\n",
    "ax3.set_xlabel(\"Predicted Label\", fontsize=12)\n",
    "\n",
    "# ============================================================================\n",
    "# CHART 4: Simple bar chart showing F1-Score for each model\n",
    "# Located at position [1, 1] (bottom-right)\n",
    "# ============================================================================\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Define colors for each bar (gray, blue, green)\n",
    "colors = [\"#808080\", \"#3498db\", \"#2ecc71\"]\n",
    "\n",
    "# Create bar chart with F1-scores\n",
    "# alpha=0.8: Make bars slightly transparent\n",
    "bars = ax4.bar(results_df[\"Model\"], results_df[\"F1-Score\"], color=colors, alpha=0.8)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax4.set_ylabel(\"F1-Score\", fontsize=12)\n",
    "ax4.set_title(\"F1-Score Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "ax4.set_ylim([0, 1])\n",
    "\n",
    "# Add grid lines for y-axis\n",
    "ax4.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add text labels on top of each bar showing exact F1-score value\n",
    "for bar in bars:\n",
    "\t# Get the height of this bar (which is the F1-score)\n",
    "\theight = bar.get_height()\n",
    "\t\n",
    "\t# Place text above the bar\n",
    "\t# bar.get_x() + bar.get_width()/2: Center horizontally on the bar\n",
    "\t# height: Place at the top of the bar\n",
    "\t# ha=\"center\": Horizontal alignment center\n",
    "\t# va=\"bottom\": Vertical alignment bottom (so text sits above bar)\n",
    "\tax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "\t\t\tround(height, 4),\n",
    "\t\t\tha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "# Adjust spacing between subplots so they don't overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the entire figure as a high-resolution PNG file\n",
    "# dpi=300: High resolution for print quality\n",
    "# bbox_inches=\"tight\": Remove extra whitespace around figure\n",
    "plt.savefig(\"model_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'model_comparison.png'\")\n",
    "print(\"Include this figure in your report!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfd8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
